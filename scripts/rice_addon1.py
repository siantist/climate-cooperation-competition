
def general_attention(self):
    # encoder representation of different words
    




# https://medium.com/hackernoon/building-a-feedforward-neural-network-from-scratch-in-python-d3526457156b
# build weight matrix 
def attention_mechanism(self):
    pai = 1/(len(self/actions_nvec)) # P(A_i)
    
    from tf.keras import Sequential
    from tf.keras.layers import Dense
    
    nlen = len(self.relevant_actions)
    # define a, feedforward network
    a_model = Sequential() # NN()
    a_model.add(Dense())
    model.add(Dense(10, activation="softmax"))
    np.dot()
    # a is the identity 
    res= a(dot_prod)
